# related work 
- Exploring Versatile Generative Language Model Via Parameter-Efficient
Transfer Learning https://arxiv.org/pdf/2004.03829.pdf
  - fineutne one model on multitple generation tasks with adapter layers 
  - they considered task-embeddings and adapter layers specific for each task 
  - they have a look up table for the tasks, and choose the corresponding
    adapter per task
  - the task-embedding is based on representation of multiple segments of the
    input 
  - they obtained worst results than GPT-2
  - very short study, task-embeddings are very different from our case, and
    only applies to generation tasks 
- UDapter: Language Adaptation for Truly Universal Dependency Parsing https://arxiv.org/pdf/2004.14327.pdf 
  - study adapters for parsing task
  - weights of adapters in the encoder and biaffine attention are generated by the dot product of language embeddings with linear layers 
  - show better results than intializing langauge embedding with random and
    learning them and becomes better for zero-shot performance
  - differences with ours: we show this architecture works for generating weights based on
    task-embedding for enc/dec models 
- Contextual Parameter Generation for Universal Neural Machine Translation https://arxiv.org/pdf/1808.08493.pdf 
  - It learns language embeddings as a context for translation and uses them
to generate the parameters of a shared translation
model for all language pairs.
  - there is no bias for the CPG network 
  - for the new language they learn embedding of the language + word embedding,
    fixing the parameters shared between the languages, and they obtain
    zero-shot performance as well  
- * Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions
  - they generate the parameters of the network given input task description
    from wikipedia, and a pretrained model, they argue that this is important to
    generate all parameters of the network, they show some zero-shot
    performance, which was not promising but better than a handful of baselines 
  - they argue for generating classifier layer from input descriptions   
- AdapterDrop: On the Efficiency of Adapters in Transformers
  - removing adapter layers from lower-transformer layers during training and
    inference to speed up 

# related work TODO list:
- udapter in details
- cgp in details 
- The Adapter-Bot: All-In-One Controllable Conversational Model
- MinTL: Minimalist Transfer Learning for Task-Oriented
Dialogue Systems
- ADAPT-AND-ADJUST: OVERCOMING THE LONG-TAIL
PROBLEM OF MULTILINGUAL SPEECH RECOGNITION
- Contextual Parameter Generation for Universal Neural Machine Translation
- CondConv: Conditionally Parameterized
Convolutions for Efficient Inference
- Improving Zero-shot Translation with Language-Independent Constraints
- Adaptive Parameterization for Neural Dialogue Generation
- improving Massively Multilingual Neural Machine Translation and
Zero-Shot Translation
- K-ADAPTER: INFUSING KNOWLEDGE INTO PRETRAINED MODELS WITH ADAPTERS
- Common Sense or World Knowledge? Investigating Adapter-Based
Knowledge Injection into Pretrained Transformers
- BERT and PALs: Projected Attention Layers for
Efficient Adaptation in Multi-Task Learning
- dynamic convolution: attention over convolution kernels 
- flexible few-shot learning of contextual similarity 

# TODO:
- get the current results 
- failed ones rerun 
- send the results to neil also 
- based on *
  - generating weight of classifier based on input task-embeddings ?
  - making all parameters including layer-norms task-embeding generated? 

- how do they do zero-shot in CPG paper?
- references in the few-shot paper, the most important ones 
- read dapter experiments, do they have zero-shot results? 

# m1-meta-task-no-relu
removed nonlinearities, and task-embeddings are now fixed 

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.0003 |         0.731416 |        0.849872 |           0.883346 |
|                  100 |          0.003  |         0.729275 |        0.875137 |           0.908672 |
|                  100 |          0.01   |         0.731416 |        0.87587  |           0.902533 |
|                  100 |          0.03   |         0.736311 |        0.877517 |           0.916347 |  **
|                  100 |          0.3    |         0.738146 |        0.878067 |           0.9033   |

#m2-meta-task-no-relu
removed nonlinearities and task-embeddings are now fixed 
# results on cola can even get better 
|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.0003 |        0.736084 |        0.768277 |                 0.906471 |
|                  100 |          0.003  |        0.75048  |        0.785179 |                 0.908655 |
|                  100 |          0.01   |        0.767754 |        0.785479 |                 0.908945 |
|                  100 |          0.03   |        0.768714 |        0.786879 |                 0.908419 |  **
|                  100 |          0.3    |        0.753359 |        0.786679 |                 0.908813 |

# m1-meta-task-no-relu-reorder
reorder the task embeddings 

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.0003 |         0.723769 |        0.85152  |           0.894091 |
|                  100 |          0.003  |         0.734475 |        0.873673 |           0.90944  |
|                  100 |          0.01   |         0.741817 |        0.879348 |           0.916347 |
|                  100 |          0.03   |         0.738758 |        0.878982 |           0.896393 |
|                  100 |          0.3    |         0.731416 |        0.885573 |           0.91251  |

# m2-meta-task-no-relu-reorder
reorder the task embeddings 

|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.0003 |        0.734165 |        0.769377 |                 0.905603 |
|                  100 |          0.003  |        0.756238 |        0.777078 |                 0.908629 |
|                  100 |          0.01   |        0.772553 |        0.782078 |                 0.90905  |
|                  100 |          0.03   |        0.769674 |        0.787479 |                 0.910498 |
|                  100 |          0.3    |        0.743762 |        0.785779 |                 0.909971 |


#
1000 samples - 800 epochs => we need at least around 12500 steps 
setting lm-head to true
12/12/2020 22:16:11 - INFO - __main__ -     snli_eval_loss = 308.1576232910156
12/12/2020 22:16:11 - INFO - __main__ -     snli_eval_acc = 0.7222722272227222
12/12/2020 22:16:11 - INFO - __main__ -     snli_epoch = 800.0

# train our model with lm_heads not freezed
# finetune both our models and t5 with the proper number of steps 
12/12/2020 22:47:37 - INFO - __main__ -     snli_eval_loss = 311.7092590332031
12/12/2020 22:47:37 - INFO - __main__ -     snli_eval_acc = 0.7328732873287329
12/12/2020 22:47:37 - INFO - __main__ -     snli_epoch = 800.0


# I ran t5-finetune and mixture1 evaluated on mixture2 for 140000 steps 
# for different number of samples
# only linear layer finetuning, fixing all parameters for the new task? 
