# TODO:
- alternative updated of task, hyper-net
- input based architecture of the model 
- get the current results 
- send the results to neil also 
- based on *
  - generating weight of classifier based on input task-embeddings ?
  - making all parameters including layer-norms task-embeding generated? 
- how do they do zero-shot in CPG paper?
- references in the few-shot paper, the most important ones 
- read udapter experiments, do they have zero-shot results? 

# related work 
- Exploring Versatile Generative Language Model Via Parameter-Efficient
Transfer Learning https://arxiv.org/pdf/2004.03829.pdf
  - fineutne one model on multitple generation tasks with adapter layers 
  - they considered task-embeddings and adapter layers specific for each task 
  - they have a look up table for the tasks, and choose the corresponding
    adapter per task
  - the task-embedding is based on representation of multiple segments of the
    input 
  - they obtained worst results than GPT-2
  - very short study, task-embeddings are very different from our case, and
    only applies to generation tasks 
- UDapter: Language Adaptation for Truly Universal Dependency Parsing https://arxiv.org/pdf/2004.14327.pdf 
  - study adapters for parsing task
  - weights of adapters in the encoder and biaffine attention are generated by the dot product of language embeddings with linear layers 
  - show better results than intializing langauge embedding with random and
    learning them and becomes better for zero-shot performance
  - differences with ours: we show this architecture works for generating weights based on
    task-embedding for enc/dec models 
- Contextual Parameter Generation for Universal Neural Machine Translation https://arxiv.org/pdf/1808.08493.pdf 
  - It learns language embeddings as a context for translation and uses them
to generate the parameters of a shared translation
model for all language pairs.
  - there is no bias for the CPG network 
  - for the new language they learn embedding of the language + word embedding,
    fixing the parameters shared between the languages, and they obtain
    zero-shot performance as well  
- * Language to Network: Conditional Parameter Adaptation with Natural Language Descriptions
  - they generate the parameters of the network given input task description
    from wikipedia, and a pretrained model, they argue that this is important to
    generate all parameters of the network, they show some zero-shot
    performance, which was not promising but better than a handful of baselines 
  - they argue for generating classifier layer from input descriptions   
- AdapterDrop: On the Efficiency of Adapters in Transformers
  - removing adapter layers from lower-transformer layers during training and
    inference to speed up

# related work, not directly related to ours 
- Common Sense or World Knowledge? Investigating Adapter-Based
Knowledge Injection into Pretrained Transformers
  -  learning adapter layers to capture knowledge from ConceptNet and its corresponding Open Mind Common Sense (OMCS) corpus 
  - very similar to k-adapter so I skip

# related work TODO list:
- Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data
- udapter in details
- cgp in details 
- The Adapter-Bot: All-In-One Controllable Conversational Model
- MinTL: Minimalist Transfer Learning for Task-Oriented
Dialogue Systems
- ADAPT-AND-ADJUST: OVERCOMING THE LONG-TAIL
PROBLEM OF MULTILINGUAL SPEECH RECOGNITION
- Contextual Parameter Generation for Universal Neural Machine Translation
- CondConv: Conditionally Parameterized
Convolutions for Efficient Inference
- Improving Zero-shot Translation with Language-Independent Constraints
- Adaptive Parameterization for Neural Dialogue Generation
- improving Massively Multilingual Neural Machine Translation and
Zero-Shot Translation
- K-ADAPTER: INFUSING KNOWLEDGE INTO PRETRAINED MODELS WITH ADAPTERS
- BERT and PALs: Projected Attention Layers for
Efficient Adaptation in Multi-Task Learning
- dynamic convolution: attention over convolution kernels 
- flexible few-shot learning of contextual similarity  and its related work 

# 14/12/2020
# Removing nonlinearities 
## mixture 1
- Removing all the nonlinearities from the hyper-network.
- each hyper-network is only a MLP layer which can be converted to one linear layer 
- task embeddings are now fixed and initialized by computing the average encoder representation over 100 samples from training set 

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.0003 |         0.731416 |        0.849872 |           0.883346 |
|                  100 |          0.003  |         0.729275 |        0.875137 |           0.908672 |
|                  100 |          0.01   |         0.731416 |        0.87587  |           0.902533 |
|                  100 |          0.03   |         0.736311 |        0.877517 |           0.916347 | 
|                  100 |          0.3    |         0.738146 |        0.878067 |           0.9033   |

- observation: 
   - comparing the best results of this model with the model with nonlinearities
   - this model

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.03   |         0.736311 |        0.877517 |           0.916347 | 

- model with nonlinearities 

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.03   |         0.741205 |        0.883742 |           0.889486 |  


## mixture 2
- the same experiment as above on mixture 2 with removing nonlinearities and task-embeddings are now fixed 

|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.0003 |        0.736084 |        0.768277 |                 0.906471 |
|                  100 |          0.003  |        0.75048  |        0.785179 |                 0.908655 |
|                  100 |          0.01   |        0.767754 |        0.785479 |                 0.908945 |
|                  100 |          0.03   |        0.768714 |        0.786879 |                 0.908419 | 
|                  100 |          0.3    |        0.753359 |        0.786679 |                 0.908813 |

- observation, comparing these results with the model with nonlinearities:
- this model:

|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.03   |        0.768714 |        0.786879 |                 0.908419 | 

- model with nonlinerity:

|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.03   |        0.761036 |        0.78857 |                 0.910366  | 

# Reordering the task-embeddings 
- reorder the task embeddings to assign different task-embeddings to each task. 
- still using the model without nonlinearities 
## mixture 1

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.0003 |         0.723769 |        0.85152  |           0.894091 |
|                  100 |          0.003  |         0.734475 |        0.873673 |           0.90944  |
|                  100 |          0.01   |         0.741817 |        0.879348 |           0.916347 |
|                  100 |          0.03   |         0.738758 |        0.878982 |           0.896393 |
|                  100 |          0.3    |         0.731416 |        0.885573 |           0.91251  |

## mixture 2

|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.0003 |        0.734165 |        0.769377 |                 0.905603 |
|                  100 |          0.003  |        0.756238 |        0.777078 |                 0.908629 |
|                  100 |          0.01   |        0.772553 |        0.782078 |                 0.90905  |
|                  100 |          0.03   |        0.769674 |        0.787479 |                 0.910498 |
|                  100 |          0.3    |        0.743762 |        0.785779 |                 0.909971 |

- observation
   - without reordering task-embeddings  

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.03   |         0.736311 |        0.877517 |           0.916347 | 


|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.03   |        0.768714 |        0.786879 |                 0.908419 | 

   - with reordering task-embeddings 

|   task_embedding_dir |   learning_rate |   boolq_eval_acc |   qnli_eval_acc |   scitail_eval_acc |
|---------------------:|----------------:|-----------------:|----------------:|-------------------:|
|                  100 |          0.01   |         0.741817 |        0.879348 |           0.916347 |


|   task_embedding_dir |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|---------------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  100 |          0.01   |        0.772553 |        0.782078 |                 0.90905  |


#
1000 samples - 800 epochs => we need at least around 12500 steps 
setting lm-head to true
12/12/2020 22:16:11 - INFO - __main__ -     snli_eval_loss = 308.1576232910156
12/12/2020 22:16:11 - INFO - __main__ -     snli_eval_acc = 0.7222722272227222
12/12/2020 22:16:11 - INFO - __main__ -     snli_epoch = 800.0

# train our model with lm_heads not freezed
# finetune both our models and t5 with the proper number of steps 
12/12/2020 22:47:37 - INFO - __main__ -     snli_eval_loss = 311.7092590332031
12/12/2020 22:47:37 - INFO - __main__ -     snli_eval_acc = 0.7328732873287329
12/12/2020 22:47:37 - INFO - __main__ -     snli_epoch = 800.0


# I ran t5-finetune and mixture1 evaluated on mixture2 for 140000 steps 
# for different number of samples
# only linear layer finetuning, fixing all parameters for the new task?

# this is when not loading from checkpoints 
|   unfreeze_lm_head |   n_finetune |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|-------------------:|-------------:|----------------:|----------------:|----------------:|-------------------------:|
|                  0 |          100 |           0.03  |        0.634357 |        0.621862 |                 0.741493 |
|                  0 |         1000 |           0.003 |        0.690019 |        0.728173 |                 0.870865 |
|                  0 |         2000 |           0.003 |        0.704415 |        0.751675 |                 0.888865 |
|                  0 |         2000 |           0.03  |        0.705374 |        0.749875 |                 0.895445 |
|                  0 |         4000 |           0.03  |        0.738964 |        0.773277 |                 0.905971 |
|                  0 |         4000 |           0.3   |        0.705374 |        0.774477 |                 0.903919 |
|                  1 |          100 |           0.03  |        0.663148 |        0.541354 |                 0.782573 |
|                  1 |          500 |           0.03  |        0.595969 |        0.617662 |                 0.815758 |
|                  1 |         4000 |           0.03  |        0.643954 |        0.732973 |                 0.886286 |
|   unfreeze_lm_head |   n_finetune |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|-------------------:|-------------:|----------------:|----------------:|-------------------------:|
|                  0 |          100 |        0.634357 |        0.621862 |                 0.741493 |
|                  0 |         1000 |        0.690019 |        0.728173 |                 0.870865 |
|                  0 |         2000 |        0.705374 |        0.751675 |                 0.895445 |
|                  0 |         4000 |        0.738964 |        0.774477 |                 0.905971 |
|                  1 |          100 |        0.663148 |        0.541354 |                 0.782573 |
|                  1 |          500 |        0.595969 |        0.617662 |                 0.815758 |
|                  1 |         4000 |        0.643954 |        0.732973 |                 0.886286 |
m1-t5-v
|   n_finetune |   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|-------------:|----------------:|----------------:|----------------:|-------------------------:|
|          100 |          0.0003 |        0.642994 |      0.631163   |                 0.750572 |
|          100 |          0.003  |        0.592131 |      0.423542   |                 0.707913 |
|          100 |          0.3    |        0        |      0.323432   |                 0.500013 |
|          500 |          0.0003 |        0.668906 |      0.691469   |                 0.836364 |
|          500 |          0.003  |        0.644914 |      0.556356   |                 0.798179 |
|          500 |          0.01   |        0.690979 |      0.126913   |                 0.500013 |
|          500 |          0.3    |        0        |      0          |                 0.500013 |
|         1000 |          0.0003 |        0.68618  |      0.726873   |                 0.874207 |
|         1000 |          0.003  |        0.648752 |      0.623862   |                 0.830996 |
|         1000 |          0.01   |        0.690979 |      0.332933   |                 0.783073 |
|         1000 |          0.3    |        0.690979 |      0.00310031 |                 0        |
|         2000 |          0.0003 |        0.723608 |      0.749875   |                 0.886155 |
|         2000 |          0.003  |        0.667946 |      0.680268   |                 0.865891 |
|         2000 |          0.01   |        0.309021 |      0.327833   |                 0.499987 |
|         2000 |          0.03   |        0        |      0          |                 0.500013 |
|         2000 |          0.3    |        0.690979 |      0          |                 0        |
|         4000 |          0.0003 |        0.744722 |      0.776178   |                 0.901418 |
|         4000 |          0.003  |        0.692898 |      0.726273   |                 0.889339 |
|         4000 |          0.01   |        0.690979 |      0.372337   |                 0.500013 |
|         4000 |          0.03   |        0.309021 |      0          |                 0        |
|   n_finetune |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|-------------:|----------------:|----------------:|-------------------------:|
|          100 |        0.642994 |        0.631163 |                 0.750572 |
|          500 |        0.690979 |        0.691469 |                 0.836364 |
|         1000 |        0.690979 |        0.726873 |                 0.874207 |
|         2000 |        0.723608 |        0.749875 |                 0.886155 |
|         4000 |        0.744722 |        0.776178 |                 0.901418 |


# lm head set to true
m1-meta-task-no-relu-lm
|   learning_rate |   qnli_eval_acc |   scitail_eval_acc |   boolq_eval_acc |
|----------------:|----------------:|-------------------:|-----------------:|
|          0.0003 |        0.855731 |           0.879509 |         0.731416 |
|          0.003  |        0.874039 |           0.90637  |         0.729275 |
|          0.01   |        0.877151 |           0.902533 |         0.732028 |
|          0.03   |        0.873123 |           0.902533 |         0.721016 |
|          0.3    |        0.803918 |           0.808903 |         0.697767 |
qnli_eval_acc    0.877151
scitail_eval_acc    0.90637
boolq_eval_acc    0.732028


m2-meta-task-no-relu-lm
|   learning_rate |   cola_eval_acc |   snli_eval_acc |   yelp_polarity_eval_acc |
|----------------:|----------------:|----------------:|-------------------------:|
|          0.0003 |        0.730326 |        0.768677 |                 0.907182 |
|          0.003  |        0.770633 |        0.778978 |                 0.90834  |
|          0.01   |        0.767754 |        0.786579 |                 0.904813 |
|          0.03   |        0.760077 |        0.775278 |                 0.889997 |
|          0.3    |        0.682342 |        0.721572 |                 0.862838 |
cola_eval_acc    0.770633
snli_eval_acc    0.786579
yelp_polarity_eval_acc    0.90834

