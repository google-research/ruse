from collections import OrderedDict
from logging import getLogger
from typing import Callable, Dict, List, Tuple

import numpy as np
from rouge_score import rouge_scorer, scoring
from sacrebleu import corpus_bleu
from transformers import EvalPrediction, PreTrainedTokenizer

from seq2seq.data import TASK_MAPPING
from seq2seq.utils import lmap
from .sentence_splitter import add_newline_to_end_of_each_sentence

logger = getLogger(__name__)


def calculate_bleu(output_lns, refs_lns, **kwargs) -> dict:
  """Uses sacrebleu's corpus_bleu implementation."""
  return {"bleu": round(corpus_bleu(output_lns, [refs_lns], **kwargs).score, 4)}


ROUGE_KEYS = ["rouge1", "rouge2", "rougeL", "rougeLsum"]


def extract_rouge_mid_statistics(dct):
  new_dict = {}
  for k1, v1 in dct.items():
    mid = v1.mid
    new_dict[k1] = {stat: round(getattr(mid, stat), 4) for stat in
                    ["precision", "recall", "fmeasure"]}
  return new_dict


def calculate_rouge(
    pred_lns: List[str],
    tgt_lns: List[str],
    use_stemmer=True,
    rouge_keys=ROUGE_KEYS,
    return_precision_and_recall=False,
    bootstrap_aggregation=True,
    newline_sep=True,
) -> Dict:
  """Calculate rouge using rouge_scorer package.

  Args:
      pred_lns: list of summaries generated by model
      tgt_lns: list of groundtruth summaries (e.g. contents of val.target)
      use_stemmer:  Bool indicating whether Porter stemmer should be used to
      strip word suffixes to improve matching.
      rouge_keys:  which metrics to compute, defaults to rouge1, rouge2, rougeL, rougeLsum
      return_precision_and_recall: (False) whether to also return precision and recall.
      bootstrap_aggregation: whether to do the typical bootstrap resampling of scores.
      Defaults to True, if False this function returns a collections.defaultdict[metric:
      list of values for each observation for each subscore]``
      newline_sep:(default=True) whether to add newline between sentences. This is essential
      for calculation rougeL
      on multi sentence summaries (CNN/DM dataset).

  Returns:
       Dict[score: value] if aggregate else defaultdict(list) keyed by rouge_keys

  """
  scorer = rouge_scorer.RougeScorer(rouge_keys, use_stemmer=use_stemmer)
  aggregator = scoring.BootstrapAggregator()
  for pred, tgt in zip(tgt_lns, pred_lns):
    # rougeLsum expects "\n" separated sentences within a summary
    if newline_sep:
      pred = add_newline_to_end_of_each_sentence(pred)
      tgt = add_newline_to_end_of_each_sentence(tgt)
    scores = scorer.score(pred, tgt)
    aggregator.add_scores(scores)

  if bootstrap_aggregation:
    result = aggregator.aggregate()
    if return_precision_and_recall:
      return extract_rouge_mid_statistics(result)  # here we return dict
    else:
      return {k: round(v.mid.fmeasure * 100, 4) for k, v in result.items()}

  else:
    return aggregator._scores  # here we return defaultdict(list)


def calculate_accuracy(output_lns, refs_lns) -> dict:
  """Computes the avarage accuracy."""
  return {"acc": (np.array(output_lns) == np.array(refs_lns)).mean()}


def build_compute_metrics_fn(task_names: List[str],
                             tokenizer: PreTrainedTokenizer) -> Callable[[EvalPrediction], Dict]:
  """Builds a dictionary from each task to the task metric."""

  def non_pad_len(tokens: np.ndarray) -> int:
    return np.count_nonzero(tokens != tokenizer.pad_token_id)

  def decode_pred(pred: EvalPrediction) -> Tuple[List[str], List[str]]:
    pred_str = tokenizer.batch_decode(pred.predictions, skip_special_tokens=True)
    label_str = tokenizer.batch_decode(pred.label_ids, skip_special_tokens=True)
    pred_str = lmap(str.strip, pred_str)
    label_str = lmap(str.strip, label_str)
    return pred_str, label_str

  def summarization_metrics(pred: EvalPrediction) -> Dict:
    pred_str, label_str = decode_pred(pred)
    rouge: Dict = calculate_rouge(pred_str, label_str)
    summ_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)
    rouge.update({"gen_len": summ_len})
    return rouge

  def translation_metrics(pred: EvalPrediction) -> Dict:
    pred_str, label_str = decode_pred(pred)
    bleu: Dict = calculate_bleu(pred_str, label_str)
    gen_len = np.round(np.mean(lmap(non_pad_len, pred.predictions)), 1)
    bleu.update({"gen_len": gen_len})
    return bleu

  def classification_metrics(pred: EvalPrediction) -> Dict:
    pred_str, label_str = decode_pred(pred)
    acc: Dict = calculate_accuracy(pred_str, label_str)
    return acc

  def tasks_metrics(task=None) -> Dict:
    category = TASK_MAPPING[task].task.category
    compute_metrics_fn = CATEGORY_EVALUATION_MAPPING[category]
    logger.info(f"selected metric {compute_metrics_fn} for task {task}")
    return compute_metrics_fn

  CATEGORY_EVALUATION_MAPPING = OrderedDict(
    [('summarization', summarization_metrics),
     ('translation', translation_metrics),
     ('classification', classification_metrics)
     ]
  )
  task_to_compute_metrics = {task: tasks_metrics(task) for task in task_names}
  return task_to_compute_metrics
